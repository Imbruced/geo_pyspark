{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from geo_pyspark.register import GeoSparkRegistrator\n",
    "from geo_pyspark.utils import GeoSparkKryoRegistrator, KryoSerializer\n",
    "from geo_pyspark.register import upload_jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_jars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "        master(\"local[*]\").\\\n",
    "        appName(\"TestApp\").\\\n",
    "        config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "        config(\"spark.kryo.registrator\", GeoSparkKryoRegistrator.getName) .\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeoSparkRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry Constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     arealandmark|\n",
      "+-----------------+\n",
      "|POINT (1.1 101.1)|\n",
      "|POINT (2.1 102.1)|\n",
      "|POINT (3.1 103.1)|\n",
      "|POINT (4.1 104.1)|\n",
      "|POINT (5.1 105.1)|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_csv_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df.createOrReplaceTempView(\"pointtable\")\n",
    "\n",
    "point_df = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)), cast(pointtable._c1 as Decimal(24,20))) as arealandmark from pointtable\")\n",
    "point_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            name|         countyshape|\n",
      "+----------------+--------------------+\n",
      "|   Cuming County|POLYGON ((-97.019...|\n",
      "|Wahkiakum County|POLYGON ((-123.43...|\n",
      "|  De Baca County|POLYGON ((-104.56...|\n",
      "|Lancaster County|POLYGON ((-96.910...|\n",
      "| Nuckolls County|POLYGON ((-98.273...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_wkt_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/county_small.tsv\")\n",
    "\n",
    "polygon_wkt_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select polygontable._c6 as name, ST_GeomFromText(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromWKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            name|         countyshape|\n",
      "+----------------+--------------------+\n",
      "|   Cuming County|POLYGON ((-97.019...|\n",
      "|Wahkiakum County|POLYGON ((-123.43...|\n",
      "|  De Baca County|POLYGON ((-104.56...|\n",
      "|Lancaster County|POLYGON ((-96.910...|\n",
      "| Nuckolls County|POLYGON ((-98.273...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_wkb_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/county_small_wkb.tsv\")\n",
    "\n",
    "polygon_wkb_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select polygontable._c6 as name, ST_GeomFromWKB(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromGeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         countyshape|\n",
      "+--------------------+\n",
      "|POLYGON ((-87.621...|\n",
      "|POLYGON ((-85.719...|\n",
      "|POLYGON ((-86.000...|\n",
      "|POLYGON ((-86.574...|\n",
      "|POLYGON ((-85.382...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_json_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/testPolygon.json\")\n",
    "\n",
    "polygon_json_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select ST_GeomFromGeoJSON(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Join - Distance Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "DistanceJoin pointshape1#181: geometry, pointshape2#197: geometry, 2.0, false\n",
      ":- Project [st_point(cast(_c0#177 as decimal(24,20)), cast(_c1#178 as decimal(24,20))) AS pointshape1#181]\n",
      ":  +- *(1) FileScan csv [_c0#177,_c1#178] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/pawel/Desktop/forked/clone_geo/GeoSpark/python/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string>\n",
      "+- Project [st_point(cast(_c0#193 as decimal(24,20)), cast(_c1#194 as decimal(24,20))) AS pointshape2#197]\n",
      "   +- *(2) FileScan csv [_c0#193,_c1#194] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/pawel/Desktop/forked/clone_geo/GeoSpark/python/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string>\n",
      "+-----------------+-----------------+\n",
      "|      pointshape1|      pointshape2|\n",
      "+-----------------+-----------------+\n",
      "|POINT (1.1 101.1)|POINT (1.1 101.1)|\n",
      "|POINT (1.1 101.1)|POINT (2.1 102.1)|\n",
      "|POINT (2.1 102.1)|POINT (1.1 101.1)|\n",
      "|POINT (2.1 102.1)|POINT (2.1 102.1)|\n",
      "|POINT (2.1 102.1)|POINT (3.1 103.1)|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_csv_df_1 = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df_1.createOrReplaceTempView(\"pointtable\")\n",
    "\n",
    "point_df1 = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape1 from pointtable\")\n",
    "point_df1.createOrReplaceTempView(\"pointdf1\")\n",
    "\n",
    "point_csv_df2 = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df2.createOrReplaceTempView(\"pointtable\")\n",
    "point_df2 = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape2 from pointtable\")\n",
    "point_df2.createOrReplaceTempView(\"pointdf2\")\n",
    "\n",
    "distance_join_df = spark.sql(\"select * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) < 2\")\n",
    "distance_join_df.explain()\n",
    "distance_join_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples please refer to http://geospark.datasyslab.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting GeoPandas to GeoSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"data/gis_osm_pois_free_1.shp\")\n",
    "\n",
    "osm_points = spark.createDataFrame(\n",
    "    gdf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- osm_id: string (nullable = true)\n",
      " |-- code: long (nullable = true)\n",
      " |-- fclass: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osm_points.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------+--------------------+\n",
      "|  osm_id|code|   fclass|          name|            geometry|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "|26860257|2422|camp_site|      de Kroon|POINT (15.3393145...|\n",
      "|26860294|2406|   chalet|Leśne Ustronie|POINT (14.8709625...|\n",
      "|29947493|2402|    motel|          null|POINT (15.0946636...|\n",
      "|29947498|2602|      atm|          null|POINT (15.0732014...|\n",
      "|29947499|2401|    hotel|          null|POINT (15.0696777...|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osm_points.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_points.createOrReplaceTempView(\"points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT osm_id,\n",
    "               code,\n",
    "               fclass,\n",
    "               name,\n",
    "               ST_Transform(geometry, 'epsg:4326', 'epsg:2180') as geom \n",
    "        FROM points\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------+--------------------+\n",
      "|  osm_id|code|   fclass|          name|                geom|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "|26860257|2422|camp_site|      de Kroon|POINT (250776.778...|\n",
      "|26860294|2406|   chalet|Leśne Ustronie|POINT (221076.709...|\n",
      "|29947493|2402|    motel|          null|POINT (233902.541...|\n",
      "|29947498|2602|      atm|          null|POINT (232447.203...|\n",
      "|29947499|2401|    hotel|          null|POINT (232208.377...|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.createOrReplaceTempView(\"points_2180\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_within_1000m = spark.sql(\"\"\"\n",
    "        SELECT a.osm_id AS id_1,\n",
    "               b.osm_id AS id_2,\n",
    "               a.geom \n",
    "        FROM points_2180 AS a, points_2180 AS b \n",
    "        WHERE ST_Distance(a.geom,b.geom) < 50\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|      id_1|      id_2|                geom|\n",
      "+----------+----------+--------------------+\n",
      "|  26860294|  26860294|POINT (221076.709...|\n",
      "|1232362450|1232362450|POINT (222167.838...|\n",
      "|1232362457|1232362457|POINT (222771.577...|\n",
      "|1232362457|1232366353|POINT (222771.577...|\n",
      "|1232362457|1232447406|POINT (222771.577...|\n",
      "|1232362457|1232469429|POINT (222771.577...|\n",
      "|1232362457|1232797546|POINT (222771.577...|\n",
      "|1232362457|3362375112|POINT (222771.577...|\n",
      "|1232362457|3578871967|POINT (222771.577...|\n",
      "|1232362457|4973313584|POINT (222771.577...|\n",
      "|1232362457|5960513485|POINT (222771.577...|\n",
      "|1232362608|1232362608|POINT (222806.339...|\n",
      "|1232362608|1232366349|POINT (222806.339...|\n",
      "|1232366344|1232366344|POINT (222329.623...|\n",
      "|1232366344|1232447366|POINT (222329.623...|\n",
      "|1232366346|1232366346|POINT (222173.050...|\n",
      "|1232366346|3589375920|POINT (222173.050...|\n",
      "|1232366349|1232362608|POINT (222812.985...|\n",
      "|1232366349|1232366349|POINT (222812.985...|\n",
      "|1232366353|1232362457|POINT (222757.202...|\n",
      "+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neighbours_within_1000m.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting GeoSpark to GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = neighbours_within_1000m.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df, geometry=\"geom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>geom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26860294</td>\n",
       "      <td>26860294</td>\n",
       "      <td>POINT (221076.710 544222.650)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1232362450</td>\n",
       "      <td>1232362450</td>\n",
       "      <td>POINT (222167.839 542112.825)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1232362457</td>\n",
       "      <td>1232362457</td>\n",
       "      <td>POINT (222771.578 542051.654)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1232362457</td>\n",
       "      <td>1232366353</td>\n",
       "      <td>POINT (222771.578 542051.654)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1232362457</td>\n",
       "      <td>1232447406</td>\n",
       "      <td>POINT (222771.578 542051.654)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65670</th>\n",
       "      <td>6465719931</td>\n",
       "      <td>6465719931</td>\n",
       "      <td>POINT (315102.147 440446.512)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65671</th>\n",
       "      <td>6618064579</td>\n",
       "      <td>6618064579</td>\n",
       "      <td>POINT (318087.175 442454.367)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65672</th>\n",
       "      <td>6618064580</td>\n",
       "      <td>6618064580</td>\n",
       "      <td>POINT (319479.742 442202.826)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65673</th>\n",
       "      <td>6819234585</td>\n",
       "      <td>6819234585</td>\n",
       "      <td>POINT (315867.379 441266.298)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65674</th>\n",
       "      <td>6819289285</td>\n",
       "      <td>6819289285</td>\n",
       "      <td>POINT (315523.786 440744.481)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65675 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id_1        id_2                           geom\n",
       "0        26860294    26860294  POINT (221076.710 544222.650)\n",
       "1      1232362450  1232362450  POINT (222167.839 542112.825)\n",
       "2      1232362457  1232362457  POINT (222771.578 542051.654)\n",
       "3      1232362457  1232366353  POINT (222771.578 542051.654)\n",
       "4      1232362457  1232447406  POINT (222771.578 542051.654)\n",
       "...           ...         ...                            ...\n",
       "65670  6465719931  6465719931  POINT (315102.147 440446.512)\n",
       "65671  6618064579  6618064579  POINT (318087.175 442454.367)\n",
       "65672  6618064580  6618064580  POINT (319479.742 442202.826)\n",
       "65673  6819234585  6819234585  POINT (315867.379 441266.298)\n",
       "65674  6819289285  6819289285  POINT (315523.786 440744.481)\n",
       "\n",
       "[65675 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
