{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{callUDF, col}\n",
    "import org.datasyslab.geospark.spatialRDD.PolygonRDD\n",
    "import org.datasyslab.geosparksql.utils.Adapter\n",
    "import org.datasyslab.geospark.enums.{FileDataSplitter, GridType}\n",
    "import org.datasyslab.geosparksql.utils.GeoSparkSQLRegistrator\n",
    "import com.vividsolutions.jts.geom.Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@3bcdf98f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@3bcdf98f"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geojsonInputLocation = /home/pkocinski001/Desktop/projects/geo_pyspark/tests/resources/testPolygon.json\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/pkocinski001/Desktop/projects/geo_pyspark/tests/resources/testPolygon.json"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val geojsonInputLocation = \"/home/pkocinski001/Desktop/projects/geo_pyspark/tests/resources/testPolygon.json\"\n",
    "\n",
    "GeoSparkSQLRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spatialRDD = org.datasyslab.geospark.spatialRDD.PolygonRDD@6db52195\n",
       "df = [geometry: string, STATEFP: string ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[geometry: string, STATEFP: string ... 9 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var spatialRDD = new PolygonRDD(spark.sparkContext, geojsonInputLocation, FileDataSplitter.GEOJSON, true)\n",
    "spatialRDD.analyze()\n",
    "var df = Adapter.toDf(spatialRDD, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatialRDD.spatialPartitioning(GridType.EQUALGRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env[-88.4418791405619 : -84.984831, 30.227773279627797 : 35.013505219053194]\n"
     ]
    }
   ],
   "source": [
    "spatialRDD.grids.toArray.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s = POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.6123 34.873337, -87.604049 34.873303, -87.604033 34.872316, -87.60415 34.867502, -87.604218 34.865687, -87.604409 34.858537, -87.604018 34.851336, -87.603716 34.844829, -87.603696 34.844307, -87.603673 34.841884, -87.60372 34.841003, -87.603879 34.838423, -87.603888 34.837682, -87.603889 34.83763, -87.613127 34.833938, -87.616451 34.832699, -87.621041 34.831431, -87.621056 34.831526, -87.62112 34.831925, -87.621603 34.8352, -87.62158 34.836087, -87.621383 34.84329, -87.621359 34.844438, -87.62129 34.846387, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\t01\t077\t011501\t5\t1500000US010770115015\t010770115015\t5\tBG\t6844991\t32636\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "POLYGON ((-87.621765 34.873444, -87.617535 34.873369, -87.6123 34.873337, -87.604049 34.873303, -87.604033 34.872316, -87.60415 34.867502, -87.604218 34.865687, -87.604409 34.858537, -87.604018 34.851336, -87.603716 34.844829, -87.603696 34.844307, -87.603673 34.841884, -87.60372 34.841003, -87.603879 34.838423, -87.603888 34.837682, -87.603889 34.83763, -87.613127 34.833938, -87.616451 34.832699, -87.621041 34.831431, -87.621056 34.831526, -87.62112 34.831925, -87.621603 34.8352, -87.62158 34.836087, -87.621383 34.84329, -87.621359 34.844438, -87.62129 34.846387, -87.62119 34.85053, -87.62144 34.865379, -87.621765 34.873444))\t01\t077\t011501\t5\t1500000US010770115015\t010770115015\t5\tBG\t6844991\t32636"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s= spatialRDD.rawSpatialRDD.collect().toArray.take(1)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.datasyslab.geosparksql.utils.GeometrySerializer\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = MapPartitionsRDD[5] at map at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at map at <console>:38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spatialRDD.rawSpatialRDD.rdd.map(\n",
    "    data => (GeometrySerializer.serialize(data))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(geom,ArrayType(ByteType,false),true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(geom,ArrayType(ByteType,false),true))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(\n",
    "    Array(StructField(\"geom\", ArrayType(ByteType, containsNull=false)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:50: error: overloaded method value createDataFrame with alternatives:\n",
       "  (data: java.util.List[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rdd: org.apache.spark.api.java.JavaRDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rdd: org.apache.spark.rdd.RDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rows: java.util.List[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame <and>\n",
       "  (rowRDD: org.apache.spark.api.java.JavaRDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame <and>\n",
       "  (rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame\n",
       " cannot be applied to (org.apache.spark.rdd.RDD[Array[Byte]], org.apache.spark.sql.types.StructType)\n",
       "       spark.createDataFrame(data, schema)\n",
       "             ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at mapPartitions at PolygonRDD.java:432"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.api.python._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:1: error: illegal start of definition\n",
       "[PythonRDD[spark].collectAndServe(spatialRDD.rawSpatialRDD)\n",
       "^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[PythonRDD[spark].collectAndServe(spatialRDD.rawSpatialRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:36: error: not found: value PythonRDDServer\n",
       "       PythonRDDServer\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PythonRDDServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
