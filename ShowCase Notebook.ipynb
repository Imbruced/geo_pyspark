{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from geo_pyspark.register import GeoSparkRegistrator\n",
    "from geo_pyspark.utils import GeoSparkKryoRegistrator, KryoSerializer\n",
    "from geo_pyspark.data import csv_point_input_location, mixed_wkt_geometry_input_location,\\\n",
    "    mixed_wkb_geometry_input_location, geojson_input_location\n",
    "from geo_pyspark.data import data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "        master(\"local[*]\").\\\n",
    "        appName(\"TestApp\").\\\n",
    "        config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "        config(\"spark.kryo.registrator\", GeoSparkKryoRegistrator.getName) .\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeoSparkRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry Constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     arealandmark|\n",
      "+-----------------+\n",
      "|POINT (1.1 101.1)|\n",
      "|POINT (2.1 102.1)|\n",
      "|POINT (3.1 103.1)|\n",
      "|POINT (4.1 104.1)|\n",
      "|POINT (5.1 105.1)|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_csv_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(csv_point_input_location)\n",
    "\n",
    "point_csv_df.createOrReplaceTempView(\"pointtable\")\n",
    "\n",
    "point_df = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)), cast(pointtable._c1 as Decimal(24,20))) as arealandmark from pointtable\")\n",
    "point_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            name|         countyshape|\n",
      "+----------------+--------------------+\n",
      "|   Cuming County|POLYGON ((-97.019...|\n",
      "|Wahkiakum County|POLYGON ((-123.43...|\n",
      "|  De Baca County|POLYGON ((-104.56...|\n",
      "|Lancaster County|POLYGON ((-96.910...|\n",
      "| Nuckolls County|POLYGON ((-98.273...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_wkt_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(mixed_wkt_geometry_input_location)\n",
    "\n",
    "polygon_wkt_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select polygontable._c6 as name, ST_GeomFromText(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromWKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            name|         countyshape|\n",
      "+----------------+--------------------+\n",
      "|   Cuming County|POLYGON ((-97.019...|\n",
      "|Wahkiakum County|POLYGON ((-123.43...|\n",
      "|  De Baca County|POLYGON ((-104.56...|\n",
      "|Lancaster County|POLYGON ((-96.910...|\n",
      "| Nuckolls County|POLYGON ((-98.273...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_wkb_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(mixed_wkb_geometry_input_location)\n",
    "\n",
    "polygon_wkb_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select polygontable._c6 as name, ST_GeomFromWKB(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromGeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         countyshape|\n",
      "+--------------------+\n",
      "|POLYGON ((-87.621...|\n",
      "|POLYGON ((-85.719...|\n",
      "|POLYGON ((-86.000...|\n",
      "|POLYGON ((-86.574...|\n",
      "|POLYGON ((-85.382...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_json_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(geojson_input_location)\n",
    "\n",
    "polygon_json_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select ST_GeomFromGeoJSON(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Join - Distance Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "DistanceJoin pointshape1#185: geometry, pointshape2#201: geometry, 2.0, false\n",
      ":- Project [st_point(cast(_c0#181 as decimal(24,20)), cast(_c1#182 as decimal(24,20))) AS pointshape1#185]\n",
      ":  +- *(1) FileScan csv [_c0#181,_c1#182] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/pawel/Desktop/geo_pyspark/geo_pyspark/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string>\n",
      "+- Project [st_point(cast(_c0#197 as decimal(24,20)), cast(_c1#198 as decimal(24,20))) AS pointshape2#201]\n",
      "   +- *(2) FileScan csv [_c0#197,_c1#198] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/pawel/Desktop/geo_pyspark/geo_pyspark/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string>\n",
      "+-----------------+-----------------+\n",
      "|      pointshape1|      pointshape2|\n",
      "+-----------------+-----------------+\n",
      "|POINT (1.1 101.1)|POINT (1.1 101.1)|\n",
      "|POINT (1.1 101.1)|POINT (2.1 102.1)|\n",
      "|POINT (2.1 102.1)|POINT (1.1 101.1)|\n",
      "|POINT (2.1 102.1)|POINT (2.1 102.1)|\n",
      "|POINT (2.1 102.1)|POINT (3.1 103.1)|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_csv_df_1 = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").load(csv_point_input_location)\n",
    "\n",
    "point_csv_df_1.createOrReplaceTempView(\"pointtable\")\n",
    "\n",
    "point_df1 = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape1 from pointtable\")\n",
    "point_df1.createOrReplaceTempView(\"pointdf1\")\n",
    "\n",
    "point_csv_df2 = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").load(csv_point_input_location)\n",
    "\n",
    "point_csv_df2.createOrReplaceTempView(\"pointtable\")\n",
    "point_df2 = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape2 from pointtable\")\n",
    "point_df2.createOrReplaceTempView(\"pointdf2\")\n",
    "\n",
    "distance_join_df = spark.sql(\"select * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) < 2\")\n",
    "distance_join_df.explain()\n",
    "distance_join_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples please refer to http://geospark.datasyslab.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting GeoPandas to GeoSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join(data_path, \"gis_osm_pois_free_1.shp\"))\n",
    "\n",
    "osm_points = spark.createDataFrame(\n",
    "    gdf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- osm_id: string (nullable = true)\n",
      " |-- code: long (nullable = true)\n",
      " |-- fclass: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osm_points.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------+--------------------+\n",
      "|  osm_id|code|   fclass|          name|            geometry|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "|26860257|2422|camp_site|      de Kroon|POINT (15.3393145...|\n",
      "|26860294|2406|   chalet|Leśne Ustronie|POINT (14.8709625...|\n",
      "|29947493|2402|    motel|          null|POINT (15.0946636...|\n",
      "|29947498|2602|      atm|          null|POINT (15.0732014...|\n",
      "|29947499|2401|    hotel|          null|POINT (15.0696777...|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osm_points.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_points.createOrReplaceTempView(\"points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT osm_id,\n",
    "               code,\n",
    "               fclass,\n",
    "               name,\n",
    "               ST_Transform(geometry, 'epsg:4326', 'epsg:2180') as geom \n",
    "        FROM points\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------+--------------------+\n",
      "|  osm_id|code|   fclass|          name|                geom|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "|26860257|2422|camp_site|      de Kroon|POINT (250776.778...|\n",
      "|26860294|2406|   chalet|Leśne Ustronie|POINT (221076.709...|\n",
      "|29947493|2402|    motel|          null|POINT (233902.541...|\n",
      "|29947498|2602|      atm|          null|POINT (232447.203...|\n",
      "|29947499|2401|    hotel|          null|POINT (232208.377...|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.createOrReplaceTempView(\"points_2180\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_within_1000m = spark.sql(\"\"\"\n",
    "        SELECT a.osm_id AS id_1,\n",
    "               b.osm_id AS id_2,\n",
    "               a.geom \n",
    "        FROM points_2180 AS a, points_2180 AS b \n",
    "        WHERE ST_Distance(a.geom,b.geom) < 50\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+\n",
      "|     id_1|      id_2|                geom|\n",
      "+---------+----------+--------------------+\n",
      "| 26860257|  26860257|POINT (250776.778...|\n",
      "| 26860294|  26860294|POINT (221076.709...|\n",
      "| 29947493|  29947493|POINT (233902.541...|\n",
      "| 29947493|3241834852|POINT (233902.541...|\n",
      "| 29947493|5964811085|POINT (233902.541...|\n",
      "| 29947498|  29947498|POINT (232447.203...|\n",
      "| 29947498|4165181885|POINT (232447.203...|\n",
      "| 29947498|5818905324|POINT (232447.203...|\n",
      "| 29947498|5846858758|POINT (232447.203...|\n",
      "| 29947499|  29947499|POINT (232208.377...|\n",
      "| 29947499|  30077461|POINT (232208.377...|\n",
      "| 29947505|  29947505|POINT (228595.321...|\n",
      "| 30077461|  29947499|POINT (232185.872...|\n",
      "| 30077461|  30077461|POINT (232185.872...|\n",
      "|269343262| 269343262|POINT (257936.165...|\n",
      "|273101780| 273101780|POINT (196825.914...|\n",
      "|310835990| 310835990|POINT (196500.614...|\n",
      "|310835990| 310841065|POINT (196500.614...|\n",
      "|310836230| 310836230|POINT (196971.397...|\n",
      "|310838954| 310838954|POINT (196748.306...|\n",
      "+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neighbours_within_1000m.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting GeoSpark to GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = neighbours_within_1000m.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df, geometry=\"geom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>geom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>26860257</td>\n",
       "      <td>26860257</td>\n",
       "      <td>POINT (250776.7780135609 504581.3320983788)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>26860294</td>\n",
       "      <td>26860294</td>\n",
       "      <td>POINT (221076.7095371484 544222.649717289)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>29947493</td>\n",
       "      <td>29947493</td>\n",
       "      <td>POINT (233902.5412607929 501298.381739473)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>29947493</td>\n",
       "      <td>3241834852</td>\n",
       "      <td>POINT (233902.5412607929 501298.381739473)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>29947493</td>\n",
       "      <td>5964811085</td>\n",
       "      <td>POINT (233902.5412607929 501298.381739473)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65670</td>\n",
       "      <td>6818416135</td>\n",
       "      <td>6818416135</td>\n",
       "      <td>POINT (260099.7586903075 458424.8084792783)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65671</td>\n",
       "      <td>6818416152</td>\n",
       "      <td>6818416152</td>\n",
       "      <td>POINT (261150.2944893772 458582.2900306303)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65672</td>\n",
       "      <td>6819234585</td>\n",
       "      <td>6819234585</td>\n",
       "      <td>POINT (315867.3786498376 441266.298256998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65673</td>\n",
       "      <td>6819289285</td>\n",
       "      <td>6819289285</td>\n",
       "      <td>POINT (315523.7861368167 440744.4805617332)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65674</td>\n",
       "      <td>6819289286</td>\n",
       "      <td>6819289286</td>\n",
       "      <td>POINT (316522.2117510386 444339.8146855617)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65675 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id_1        id_2                                         geom\n",
       "0        26860257    26860257  POINT (250776.7780135609 504581.3320983788)\n",
       "1        26860294    26860294   POINT (221076.7095371484 544222.649717289)\n",
       "2        29947493    29947493   POINT (233902.5412607929 501298.381739473)\n",
       "3        29947493  3241834852   POINT (233902.5412607929 501298.381739473)\n",
       "4        29947493  5964811085   POINT (233902.5412607929 501298.381739473)\n",
       "...           ...         ...                                          ...\n",
       "65670  6818416135  6818416135  POINT (260099.7586903075 458424.8084792783)\n",
       "65671  6818416152  6818416152  POINT (261150.2944893772 458582.2900306303)\n",
       "65672  6819234585  6819234585   POINT (315867.3786498376 441266.298256998)\n",
       "65673  6819289285  6819289285  POINT (315523.7861368167 440744.4805617332)\n",
       "65674  6819289286  6819289286  POINT (316522.2117510386 444339.8146855617)\n",
       "\n",
       "[65675 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_pyspark_env",
   "language": "python",
   "name": "geo_pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
